{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b66cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f22b5",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41a0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#### Loading the data\"\"\"\n",
    "\n",
    "orig_data = pd.read_csv('./Data/UCI-PM.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "orig_data = orig_data.drop(columns=['cbwd', 'No'])\n",
    "\n",
    "# Drop na values\n",
    "aug_data = orig_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28f9d0",
   "metadata": {},
   "source": [
    "#### Normalize and function to inverse transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1808f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#### Scaling and inverse_transform method\"\"\"\n",
    "\n",
    "# Scale the whole data\n",
    "temp_x = aug_data.values # Returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "temp_x_scaled = min_max_scaler.fit_transform(temp_x)\n",
    "df = pd.DataFrame(temp_x_scaled, columns = aug_data.columns)\n",
    "\n",
    "# Inverse_transform of PM2.5\n",
    "pm_inv_scaler = preprocessing.MinMaxScaler()\n",
    "temp_scaler = np.reshape(aug_data['pm2.5'].values, (-1,1))\n",
    "pm_inv_scaler.fit(temp_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c4b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_pm_scaler(data):\n",
    "    data = np.asarray(data)\n",
    "    data = np.reshape(data, (-1, 1))\n",
    "    return pm_inv_scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21558b43",
   "metadata": {},
   "source": [
    "#### Model Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d158d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./msslstm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24384e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to supervised learning format\n",
    "def convert_to_supervised(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        \n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and train\n",
    "def split_data(x, y, split_percentage = 0.8):\n",
    "    n_train_hours = (int)(split_percentage * len(x))\n",
    "\n",
    "    train_x = x[:n_train_hours, :]\n",
    "    train_y = y[:n_train_hours]\n",
    "\n",
    "    test_x  = x[n_train_hours:, :]\n",
    "    test_y = y[n_train_hours:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366be61b",
   "metadata": {},
   "source": [
    "#### Compute Error matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afac8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_metric(actual, predicted):\n",
    "    rmse = []\n",
    "    for i in range(len(actual[1])):\n",
    "        c_actual = inverse_pm_scaler(actual[:, i])\n",
    "        c_predicted = inverse_pm_scaler(predicted[:, i])\n",
    "        sum_error = 0.0\n",
    "        for j in range(len(c_actual)):\n",
    "            prediction_error = c_predicted[j] - c_actual[j]\n",
    "            sum_error += (prediction_error ** 2)\n",
    "        mean_error = sum_error / float(len(c_actual))\n",
    "        rmse.append(sqrt(mean_error))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ab1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_function(actual, predicted):\n",
    "    mape = []\n",
    "    for i in range(len(actual[1])):\n",
    "        c_actual = inverse_pm_scaler(actual[:, i])\n",
    "        c_predicted = inverse_pm_scaler(predicted[:, i])\n",
    "        sum_error = 0.0\n",
    "\n",
    "        for j in range(len(c_actual)):\n",
    "            # Avoiding divide by zero\n",
    "            if c_actual[j] == 0:\n",
    "                c_actual[j] = 1\n",
    "            prediction_error = np.abs((c_actual[j] - c_predicted[j])/c_actual[j])\n",
    "            sum_error += prediction_error\n",
    "        \n",
    "        mape_error = sum_error / float(len(c_actual))*100\n",
    "        mape.append(mape_error[0])\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3ff07",
   "metadata": {},
   "source": [
    "#### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a444b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_lstm(x, y, step, layers, nodes, epoch, n_features=7):\n",
    "    model_id = \"msslstm_\" + str(step) + \"_\" + str(layers) + \"_\" + str(nodes) + \"_\" + str(epoch) + '/'\n",
    "\n",
    "    # define model\n",
    "    base_model = Sequential()\n",
    "    base_model.add(LSTM(nodes, activation='relu', return_sequences=True, input_shape=(step, n_features)))\n",
    "\n",
    "    for i in range(1, layers):\n",
    "        base_model.add(LSTM(nodes, return_sequences=True, activation='relu'))\n",
    "\n",
    "    base_model.add(LSTM(nodes, activation='relu'))\n",
    "    base_model.add(Dense(6))\n",
    "    base_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Tensorboard\n",
    "    v_log_dir = path + \"/tb_slstm/\" + model_id + \"/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    vlstm_tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=v_log_dir, histogram_freq=1)\n",
    "\n",
    "    # fit base model\n",
    "    history = base_model.fit(x, y, callbacks=[vlstm_tensorboard_callback],validation_split=0.2, epochs=epoch, verbose=0)\n",
    "\n",
    "    return base_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ef11c",
   "metadata": {},
   "source": [
    "#### Grid-Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3077f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vlstm():\n",
    "    # choose a number of time steps\n",
    "    steps_out = 6\n",
    "\n",
    "    steps_in = [2, 4, 6, 12]\n",
    "    epochs = [100]\n",
    "    n_layers = [2, 4, 8, 16]\n",
    "    n_nodes = [32, 64, 128, 256]\n",
    "\n",
    "    temp = []\n",
    "    for step in steps_in:\n",
    "\n",
    "        x, y = convert_to_supervised(df.values, step, steps_out)\n",
    "        train_x, train_y, test_x, test_y = split_data(x, y)\n",
    "\n",
    "        for epoch in epochs:\n",
    "            for layers in n_layers:\n",
    "                for nodes in n_nodes:\n",
    "                    model_name = \"msslstm_\" + str(step) + \"_\" + str(layer) + \"_\" + str(nodes) + \"_\" + str(epoch)\n",
    "\n",
    "                    # Call method to fit the model\n",
    "                    v_model, history = stack_lstm(train_x, train_y, step, layers, nodes, epoch)\n",
    "\n",
    "                    # Calculate Train RMSE\n",
    "                    train_yhat = v_model.predict(train_x, verbose=0)\n",
    "\n",
    "                    train_rmse = rmse_metric(train_y, train_yhat)\n",
    "                    train_mape = mape_function(train_y, train_yhat)\n",
    "\n",
    "                    # Calculate Test RMSE\n",
    "                    test_yhat = v_model.predict(test_x, verbose=0)\n",
    "\n",
    "                    test_rmse = rmse_metric(test_y, test_yhat)\n",
    "                    test_mape = mape_function(train_y, train_yhat)\n",
    "\n",
    "                    temp.append([step, layers, nodes, epoch, train_rmse, train_mape, test_rmse, train_mape])\n",
    "\n",
    "                    # Save all the data locally\n",
    "                    os.makedirs(os.path.dirname(path + 'train_y/'), exist_ok=True)\n",
    "                    os.makedirs(os.path.dirname(path + 'train_yhat/'), exist_ok=True)\n",
    "                    os.makedirs(os.path.dirname(path + 'test_y/'), exist_ok=True)\n",
    "                    os.makedirs(os.path.dirname(path + 'test_yhat/'), exist_ok=True)\n",
    "                    os.makedirs(os.path.dirname(path + 'history/'), exist_ok=True)\n",
    "                    os.makedirs(os.path.dirname(path + 'model/'), exist_ok=True)\n",
    "\n",
    "                    np.save(path + 'train_y/' + model_name, test_y)\n",
    "                    np.save(path + 'train_yhat/' + model_name, test_yhat)\n",
    "                    np.save(path + 'test_y/' + model_name, test_y)\n",
    "                    np.save(path + 'test_yhat/' + model_name, test_yhat)\n",
    "                    np.save(path + 'history/' + model_name, history.history)\n",
    "                    v_model.save(path + 'model/' + model_name + '.h5')\n",
    "\n",
    "    temp_df = pd.DataFrame(temp, columns=['Step Size', 'No of layers', 'No of Nodes', 'Epoch', 'Train RMSE', 'Train MAPE', 'Test RMSE', 'Train MAPE'])\n",
    "    temp_df.to_csv(path + 'msstack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de715246",
   "metadata": {},
   "source": [
    "### Call the grid-search method and train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a001afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_vlstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
